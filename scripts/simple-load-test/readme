
Scripts for simplified load tests.

In general, for performance benchmarking it is recommended to use cosbench.

Thing with cosbench is, it requires _a lot_ of system resources to run.  To max
out single s3 server node you might need e.g. 3 cosbench client nodes.  In case
there is not enough client nodes, or it is not possible to install cosbench in
your environment, you can use these simplified workloads defined in scripts in
this folder.


## rwrite.alias

General usage:

  $ source rwrite.alias
  $ bucket=s3://t7ko files=16 workers=4 size=256 rwrite
  $ bucket=s3://t7ko files=16 workers=4 size=256 rwrite_multibucket

Alias file defines shell (bash) aliases/commands for testing.


`rwrite` command runs write workload.  Essentially, what it does is, it runs
multiple parallel `aws s3 cp` commands to upload files to a given s3 bucket.
Files are generated on the fly (in ram) using `dd` command, by reading from
/dev/zero.

Parameters for the command are specified in the form of key=value pairs in
front of the rwrite command.  For example, to specify bucket name, use
`bucket=s3://name`.  Full list of parameters:

* `bucket`  -- full bucket name with s3 prefix and no trailing slash.
* `workers` -- number of cp commands to run in parallel.
* `size`    -- size of each file (in megabytes).
* `files`   -- number of files to send in total before the test ends.


`rwrite_multibucket` is very much like `rwrite`, except that it creates
individual bucket for each file to be uploaded (this test was added to see if
such separation will help with KVS locks).


General note: both commands use aws s3 cli to send data.  An attempt to replace
with s3cmd was made, but aws cli shows better performance.  To install and
configure aws cli, follow steps outlined in the following document:

     https://docs.google.com/document/d/1uuZ3LAfpiCyt4iitpAUdvc05cwr4hp4kMcbtWLG7Xd8/edit?ts=5d43fce9#

